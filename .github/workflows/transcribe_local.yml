name: Transcribe AWS Podcasts (whisper.cpp + verified fetch + PR)

permissions:
  contents: write
  pull-requests: write

on:
  workflow_dispatch:
    inputs:
      model:
        description: "Whisper model (base.en, small.en, medium.en)"
        required: true
        default: "small.en"
      source_name:
        description: "Name to record as 'source' (e.g., AWS Official Podcast)"
        required: true
        default: "AWS Official Podcast"
      feed_url:
        description: "RSS URL to pull MP3s (AWS Official Podcast)"
        required: false
        default: "https://d3gih7jbfe3jlq.cloudfront.net/aws-podcast-feed.xml"
      max_items:
        description: "Max episodes to pull from RSS"
        required: false
        default: "5"

jobs:
  transcribe:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential ffmpeg wget python3-pip curl
          pip3 install feedparser python-dateutil

      - name: Build whisper.cpp
        run: |
          git clone https://github.com/ggerganov/whisper.cpp.git
          cd whisper.cpp && make

      - name: Pick model URL
        id: pick_model
        run: |
          MODEL="${{ github.event.inputs.model }}"
          case "$MODEL" in
            base.en)
              echo "url=https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin" >> $GITHUB_OUTPUT
              echo "file=ggml-base.en.bin" >> $GITHUB_OUTPUT
              ;;
            small.en)
              echo "url=https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small.en.bin" >> $GITHUB_OUTPUT
              echo "file=ggml-small.en.bin" >> $GITHUB_OUTPUT
              ;;
            medium.en)
              echo "url=https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-medium.en.bin" >> $GITHUB_OUTPUT
              echo "file=ggml-medium.en.bin" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "Unsupported model: $MODEL"; exit 1
              ;;
          esac

      - name: Download model (not committed)
        run: |
          mkdir -p models
          wget -q "${{ steps.pick_model.outputs.url }}" -O "models/${{ steps.pick_model.outputs.file }}"
          ls -lh models/

      - name: Prepare work dirs
        run: |
          mkdir -p audio data/transcripts

      # ---- VERIFY FEED ACCESS + LOG EVIDENCE ------------------------------
      - name: Verify RSS connectivity and list items
        env:
          FEED_URL: ${{ github.event.inputs.feed_url }}
        run: |
          python3 - << 'PY'
          import os, json, feedparser, urllib.request
          FEED = os.environ["FEED_URL"].strip()
          print(f"[VERIFY] Fetching feed: {FEED}")
          d = feedparser.parse(FEED)
          print(f"[VERIFY] Feed title: {d.feed.get('title','<no title>')}")
          print(f"[VERIFY] Entries found: {len(d.entries)}")
          # print first 10 titles + mp3 urls
          shown = 0
          for e in d.entries:
            if shown >= 10: break
            title = e.get("title","<no title>")
            mp3 = None
            for L in (getattr(e, "enclosures", []) or []):
              if (L.get("type","") or "").startswith("audio/") and (L.get("href","") or "").endswith(".mp3"):
                mp3 = L["href"]; break
            if not mp3:
              for L in (getattr(e, "links", []) or []):
                if (L.get("type","") or "").startswith("audio/") and (L.get("href","") or "").endswith(".mp3"):
                  mp3 = L["href"]; break
            print(f"[VERIFY] {title} :: MP3={mp3}")
            shown += 1
          # Try a HEAD request on first MP3 if available
          first = None
          for e in d.entries:
            for L in (getattr(e, "enclosures", []) or []):
              if (L.get("type","") or "").startswith("audio/") and (L.get("href","") or "").endswith(".mp3"):
                first = L["href"]; break
            if first: break
          if first:
            print(f"[VERIFY] Checking MP3 reachability: {first}")
            req = urllib.request.Request(first, method="HEAD", headers={"User-Agent":"Mozilla/5.0 (GitHubActions)"})
            try:
              with urllib.request.urlopen(req, timeout=30) as r:
                print(f"[VERIFY] MP3 HEAD status: {r.status}")
            except Exception as ex:
              print(f"[VERIFY] MP3 HEAD failed: {ex}")
          PY

      # ---- FETCH AUDIO WITH DATE+TITLE FILENAMES + RETRIES ----------------
      - name: Fetch audio from RSS (date-prefixed filenames, retries, UA)
        env:
          FEED_URL: ${{ github.event.inputs.feed_url }}
          MAX_ITEMS: ${{ github.event.inputs.max_items }}
        run: |
          python3 - << 'PY'
          import os, re, time, urllib.request, feedparser
          from dateutil import parser as dtp

          FEED_URL = os.environ.get("FEED_URL","").strip()
          MAX_ITEMS = int(os.environ.get("MAX_ITEMS","5") or "5")
          UA = {"User-Agent":"Mozilla/5.0 (GitHubActions WhisperFetcher)"}

          if not FEED_URL:
            print("[FETCH] No FEED_URL provided"); raise SystemExit(2)

          print(f"[FETCH] Parsing feed: {FEED_URL}")
          d = feedparser.parse(FEED_URL)
          src_title = d.feed.get("title","AWS Official Podcast")
          meta_lines = []
          count = 0

          def safe_name(s, maxlen=100):
            s = re.sub(r'[^a-zA-Z0-9._-]+','-', s or '').strip('-')
            return s[:maxlen] if s else 'episode'

          for e in d.entries:
            if count >= MAX_ITEMS: break
            # find MP3
            url = None
            for L in (getattr(e, "enclosures", []) or []):
              if (L.get("type","") or "").startswith("audio/") and (L.get("href","") or "").endswith(".mp3"):
                url = L["href"]; break
            if not url:
              for L in (getattr(e, "links", []) or []):
                if (L.get("type","") or "").startswith("audio/") and (L.get("href","") or "").endswith(".mp3"):
                  url = L["href"]; break
            if not url:
              print("[FETCH] No MP3 URL in entry, skipping.")
              continue

            title = (e.get("title","episode") or "episode").strip()
            pub = ""
            if hasattr(e, "published"):
              try:
                pub = dtp.parse(e.published).date().isoformat()
              except Exception:
                pub = e.get("published","") or ""

            fn = f"{pub}_{safe_name(title)}" if pub else safe_name(title)
            fn = fn[:120]
            out = os.path.join("audio", f"{fn}.mp3")
            print(f"[FETCH] Downloading: {title} | {url} -> {out}")

            # download with retries
            ok = False
            for attempt in range(1,4):
              try:
                req = urllib.request.Request(url, headers=UA)
                with urllib.request.urlopen(req, timeout=120) as r, open(out,"wb") as f:
                  f.write(r.read())
                ok = True
                break
              except Exception as ex:
                print(f"[FETCH] Attempt {attempt}/3 failed: {ex}")
                time.sleep(2*attempt)
            if not ok:
              print("[FETCH] Giving up on this episode.")
              continue

            meta_lines.append((src_title, title, pub, getattr(e, "link", "") or url, out))
            count += 1

          os.makedirs("data/transcripts", exist_ok=True)
          with open("data/transcripts/rss_meta.tsv","w",encoding="utf-8") as f:
            for row in meta_lines:
              f.write("\t".join(row) + "\n")
          print(f"[FETCH] Downloaded {count} episodes; metadata -> data/transcripts/rss_meta.tsv")

          if count == 0:
            print("[FETCH] ERROR: 0 audio files downloaded. Failing the job so you can see this clearly.")
            raise SystemExit(3)
          PY

      - name: Show downloaded audio files
        run: |
          echo "[AUDIO] Contents of audio/:"
          ls -lh audio || true

      - name: Transcribe with whisper.cpp (skip existing)
        env:
          SRC_DEFAULT: ${{ github.event.inputs.source_name }}
          MODEL_FILE: ${{ steps.pick_model.outputs.file }}
        run: |
          python3 - << 'PY'
          import os, csv, subprocess
          SRC_DEFAULT = os.environ.get("SRC_DEFAULT","AWS Official Podcast")
          MODEL_FILE  = os.environ["MODEL_FILE"]
          os.makedirs("data/transcripts", exist_ok=True)
          csv_path = "data/transcripts/transcripts.csv"
          csv_exists = os.path.exists(csv_path)
          out_rows = []
          if csv_exists:
            with open(csv_path, newline="", encoding="utf-8") as f:
              out_rows = list(csv.reader(f))
          else:
            out_rows.append(["source","episode_title","episode_date","episode_link","transcript_path"])

          # load metadata from fetch step
          meta = {}
          if os.path.exists("data/transcripts/rss_meta.tsv"):
            with open("data/transcripts/rss_meta.tsv", encoding="utf-8") as f:
              for line in f:
                parts = line.rstrip("\n").split("\t")
                if len(parts) == 5:
                  src,title,date,link,path = parts
                  meta[os.path.basename(path)] = (src or SRC_DEFAULT, title or os.path.splitext(os.path.basename(path))[0], date or "", link or "")

          audio_dir = "audio"
          any_audio = False
          for name in sorted(os.listdir(audio_dir) if os.path.isdir(audio_dir) else []):
            if not name.lower().endswith((".mp3",".m4a",".wav")):
              continue
            any_audio = True
            in_path = os.path.join(audio_dir, name)
            stem = os.path.splitext(name)[0]
            out_txt = os.path.join("data","transcripts", f"{stem}.txt")
            if os.path.exists(out_txt):
              print(f"[X] Skip existing transcript: {out_txt}")
              continue
            print(f"[TRX] Transcribing: {in_path}")
            cmd = [
              "./whisper.cpp/main",
              "-m", f"models/{MODEL_FILE}",
              "-f", in_path,
              "-otxt",
              "-of", os.path.join("data","transcripts", stem),
              "-ml", "1"
            ]
            res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
            print(res.stdout)
            src, title, date, link = meta.get(name, (SRC_DEFAULT, stem, "", ""))
            out_rows.append([src, title, date, link, out_txt])

          if not any_audio:
            print("[TRX] ERROR: audio/ is empty; fetch step likely failed earlier.")
            raise SystemExit(4)

          with open(csv_path, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerows(out_rows)
          print(f"[TRX] Wrote CSV: {csv_path} ({len(out_rows)-1} rows)")
          PY

      - name: Force a small change so PR always exists
        run: |
          echo "Last run: $(date -u)" > data/transcripts/.last-run

      # Only include transcripts in the PR to avoid large files
      - name: Create/Update PR with transcripts
        id: create_pr
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: bot/transcripts
          add-paths: |
            data/transcripts/**
          commit-message: "Add/Update AWS Podcast transcripts"
          title: "Add/Update AWS Podcast transcripts"
          body: "Automated transcript update from workflow run."
          labels: automation

      - name: Show PR link
        run: |
          echo "PR URL: ${{ steps.create_pr.outputs['pull-request-url'] }}"
          echo "PR Number: ${{ steps.create_pr.outputs['pull-request-number'] }}"
